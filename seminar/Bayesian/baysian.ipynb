{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian\n",
    "\n",
    "## 1. 문제\n",
    "- 사람의 보편적인 인식법칙 -> ‘가장 그럴듯한’ class로 분류\n",
    "- 기계(컴퓨터)의 인식 -> 수학적 틀을 이용하여 프로그래밍이 가능\n",
    "- 기본적인 확률 기초이론을 사용\n",
    "- P(W|X) 특징 X가 주어졌을 때 Class W 에서 발생했을 확률 (사후 확률)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 파이썬으로 텍스트 분류하기\n",
    "- 온라인 전자 게시판을 위한 필터 구현\n",
    "- 작성자가 부정적이거나 폭력적인 언어를 사용할 경우 필터링 (실제 필터링을 하지 않음 분류 까지만)\n",
    "- 여기에서 2가지 범주로 나눔 1. 폭력적인, 0.ㅣ 폭력적이지 않은"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 텍스트로 단어 벡터 만들기\n",
    "- 하나의 문장(문서)을 하나의 벡터로 변환한 단어 벡터 형태로 변환이 필요\n",
    "- 1. 모든 문서에 있는 단어 고려하기 위해 중복 단어 제거 후 단어 장 생성\n",
    "- 2. 각각의 문장(문서)을 벡터로 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# loadDataSet(): 단어 리스트와 리스트안의 문서가 폭력적인지(0) 폭력적이지 않은지(1) 분류 값을 반환\n",
    "def loadDataSet():\n",
    "    postingList=[['my','dog','has','flea','problems','help','please'],\n",
    "                 ['maybe','not','take','him','to','dog','park','stupid'],\n",
    "                ['my','dalmation','is','so','cute','I','love','him'],\n",
    "                ['stop','posting','stupid','worthless','garbage'],\n",
    "                ['mr','licks','ate','my','steak','how','to','stop','him'],\n",
    "                ['quit','buying','wothless','dog','food','stupid']]\n",
    "    classVec = [0,1,0,1,0,1] # 1: 폭력적인 0: 폭력적이지 않음\n",
    "    return postingList,classVec\n",
    "\n",
    "#createVocabList(dataset) : 단어 리스트 dataSet중 중복된 것을 제거하여 리스트로 반환\n",
    "def createVocabList(dataSet):\n",
    "    vocabSet = []\n",
    "    for document in dataSet:\n",
    "        for word in document:\n",
    "            if word not in vocabSet:\n",
    "                vocabSet.append(word)\n",
    "    return vocabSet\n",
    "\n",
    "#setOfWords2Vec(vocabList, inputSet): inputSet의 단어 들이, vocabList에 있으면 1, 없으면 0인 벡터 리스트 반환#\n",
    "def setOfWords2Vec(vocabList, inputSet):\n",
    "    returnVec = [0]*len(vocabList) \n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] = 1\n",
    "        else: print \"the word: %s is not in my Vocabulary\" % word\n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'], ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'], ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'], ['stop', 'posting', 'stupid', 'worthless', 'garbage'], ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'], ['quit', 'buying', 'wothless', 'dog', 'food', 'stupid']]\n",
      "[0, 1, 0, 1, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "DocList, listClasses = loadDataSet()\n",
    "print DocList\n",
    "print listClasses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 모든 문서에 있는 단어 고려하기 위해 중복 단어 제거 후 단어 장 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my', 'dog', 'has', 'flea', 'problems', 'help', 'please', 'maybe', 'not', 'take', 'him', 'to', 'park', 'stupid', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'stop', 'posting', 'worthless', 'garbage', 'mr', 'licks', 'ate', 'steak', 'how', 'quit', 'buying', 'wothless', 'food']\n"
     ]
    }
   ],
   "source": [
    "myVocabList = createVocabList(DocList)\n",
    "print myVocabList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 각각의 문장(문서)을 벡터로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print setOfWords2Vec(myVocabList,DocList[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 단어 벡터로 확률 계산 함수 작성\n",
    "- 베이지안 분류기 훈련\n",
    "- 각 문장(문서)에서 출현하는 단어 count\n",
    "- 전체 긍정적인 문장(문서),부정적인 문장(문서) 각각에서 해당 단어가 출현하는 확률 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "\n",
    "def trainNB0(trainMatrix, trainCategory):\n",
    "    numTrainDocs = len(trainMatrix)\n",
    "    numWords = len(trainMatrix[0]) ## 각 분류 항목에 대한 문서의 개수 세기\n",
    "    pAbusive = sum(trainCategory) / float(numTrainDocs) #사전확률 계산 (폭력적이지 않은(1)에 대한)\n",
    "    p0Num = zeros(numWords); p1Num = zeros(numWords) ##초기화\n",
    "    p0Denom = 0.0; p1Denom = 0.0 \n",
    "    for i in range(numTrainDocs): ## 훈련을 위한 모든 문서의 개수만큼 반복\n",
    "        if trainCategory[i] ==1: ## 폭력적인 단어의 문서일 경우\n",
    "            p1Num +=trainMatrix[i] ## 해당문서의 단어 갯수 증가\n",
    "            p1Denom += sum(trainMatrix[i]) ## 전체 문서에서 해당 문서의 단어 갯수\n",
    "        else: ## 비폭력적인 단어의 문서일 경우\n",
    "            p0Num += trainMatrix[i]  ## 해당문서의 단어 갯수 증가\n",
    "            p0Denom += sum(trainMatrix[i])## 전체 문서에서 해당 문서의 단어 갯수\n",
    "    p1Vect = p1Num / p1Denom ## 각 폭력적인 단어들이 전체 폭력적인 단어에서 나타날 확률\n",
    "    p0Vect = p0Num / p0Denom ## 각 비폭력적인 단어들이 전체 비폭력적인 단어에서 나타날 확률\n",
    "        \n",
    "    \n",
    "    return p0Vect,p1Vect,pAbusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "from numpy import *\n",
    "\n",
    "trainMat=[]\n",
    "for Doc in DocList:\n",
    "    trainMat.append(setOfWords2Vec(myVocabList,Doc)) ## 각 5개의 문서의 단어가 전체 단어에서의 위치 혹은 존재 여부\n",
    "\n",
    "print trainMat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p0V,p1V,pAb=trainNB0(trainMat,listClasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "print pAb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my', 0.125, 0.0]\n",
      "['dog', 0.041666666666666664, 0.10526315789473684]\n",
      "['has', 0.041666666666666664, 0.0]\n",
      "['flea', 0.041666666666666664, 0.0]\n",
      "['problems', 0.041666666666666664, 0.0]\n",
      "['help', 0.041666666666666664, 0.0]\n",
      "['please', 0.041666666666666664, 0.0]\n",
      "['maybe', 0.0, 0.052631578947368418]\n",
      "['not', 0.0, 0.052631578947368418]\n",
      "['take', 0.0, 0.052631578947368418]\n",
      "['him', 0.083333333333333329, 0.052631578947368418]\n",
      "['to', 0.041666666666666664, 0.052631578947368418]\n",
      "['park', 0.0, 0.052631578947368418]\n",
      "['stupid', 0.0, 0.15789473684210525]\n",
      "['dalmation', 0.041666666666666664, 0.0]\n",
      "['is', 0.041666666666666664, 0.0]\n",
      "['so', 0.041666666666666664, 0.0]\n",
      "['cute', 0.041666666666666664, 0.0]\n",
      "['I', 0.041666666666666664, 0.0]\n",
      "['love', 0.041666666666666664, 0.0]\n",
      "['stop', 0.041666666666666664, 0.052631578947368418]\n",
      "['posting', 0.0, 0.052631578947368418]\n",
      "['worthless', 0.0, 0.052631578947368418]\n",
      "['garbage', 0.0, 0.052631578947368418]\n",
      "['mr', 0.041666666666666664, 0.0]\n",
      "['licks', 0.041666666666666664, 0.0]\n",
      "['ate', 0.041666666666666664, 0.0]\n",
      "['steak', 0.041666666666666664, 0.0]\n",
      "['how', 0.041666666666666664, 0.0]\n",
      "['quit', 0.0, 0.052631578947368418]\n",
      "['buying', 0.0, 0.052631578947368418]\n",
      "['wothless', 0.0, 0.052631578947368418]\n",
      "['food', 0.0, 0.052631578947368418]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(myVocabList)):\n",
    "    print [myVocabList[i],p0V[i],p1V[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 확률 벡터에 log를 취하도록 변경\n",
    "## 분모 값을 2.0으로 초기화\n",
    "## 이전 함수의 결함을 처리하기 위한 변경\n",
    "## 1. 확률값이 아주 작을 경우 곱셈시 언더플로우가 일어나는것을 방지\n",
    "## 2. 조건부 확률에서 0으로 나누어 지는경우 방지ㅠ\n",
    "\n",
    "def trainNB0(trainMatrix,trainCategory):\n",
    "    numTrainDocs=len(trainMatrix)\n",
    "    numWords=len(trainMatrix[0])\n",
    "    pAbusive=sum(trainCategory)/float(numTrainDocs)\n",
    "    p0Num=ones(numWords);p1Num=ones(numWords)  ##zeros->ones\n",
    "    p0Denom=2.0;p1Denom=2.0  ##0.0->2.0\n",
    "    for i in range(numTrainDocs):\n",
    "        if trainCategory[i]==1:\n",
    "            p1Num+=trainMatrix[i]\n",
    "            p1Denom+=sum(trainMatrix[i])\n",
    "        else:\n",
    "            p0Num+=trainMatrix[i]\n",
    "            p0Denom+=sum(trainMatrix[i])\n",
    "    p1Vect=log(p1Num/p1Denom)      ## p1Num/p1Denom -> log(p1Num/p1Denom)\n",
    "    p0Vect=log(p0Num/p0Denom)      ## p0Num/p0Denom -> log(p0Num/p0Denom)\n",
    "    return p0Vect,p1Vect,pAbusive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my', -1.8718021769015913, -3.044522437723423]\n",
      "['dog', -2.5649493574615367, -1.9459101490553135]\n",
      "['has', -2.5649493574615367, -3.044522437723423]\n",
      "['flea', -2.5649493574615367, -3.044522437723423]\n",
      "['problems', -2.5649493574615367, -3.044522437723423]\n",
      "['help', -2.5649493574615367, -3.044522437723423]\n",
      "['please', -2.5649493574615367, -3.044522437723423]\n",
      "['maybe', -3.2580965380214821, -2.3513752571634776]\n",
      "['not', -3.2580965380214821, -2.3513752571634776]\n",
      "['take', -3.2580965380214821, -2.3513752571634776]\n",
      "['him', -2.1594842493533721, -2.3513752571634776]\n",
      "['to', -2.5649493574615367, -2.3513752571634776]\n",
      "['park', -3.2580965380214821, -2.3513752571634776]\n",
      "['stupid', -3.2580965380214821, -1.6582280766035324]\n",
      "['dalmation', -2.5649493574615367, -3.044522437723423]\n",
      "['is', -2.5649493574615367, -3.044522437723423]\n",
      "['so', -2.5649493574615367, -3.044522437723423]\n",
      "['cute', -2.5649493574615367, -3.044522437723423]\n",
      "['I', -2.5649493574615367, -3.044522437723423]\n",
      "['love', -2.5649493574615367, -3.044522437723423]\n",
      "['stop', -2.5649493574615367, -2.3513752571634776]\n",
      "['posting', -3.2580965380214821, -2.3513752571634776]\n",
      "['worthless', -3.2580965380214821, -2.3513752571634776]\n",
      "['garbage', -3.2580965380214821, -2.3513752571634776]\n",
      "['mr', -2.5649493574615367, -3.044522437723423]\n",
      "['licks', -2.5649493574615367, -3.044522437723423]\n",
      "['ate', -2.5649493574615367, -3.044522437723423]\n",
      "['steak', -2.5649493574615367, -3.044522437723423]\n",
      "['how', -2.5649493574615367, -3.044522437723423]\n",
      "['quit', -3.2580965380214821, -2.3513752571634776]\n",
      "['buying', -3.2580965380214821, -2.3513752571634776]\n",
      "['wothless', -3.2580965380214821, -2.3513752571634776]\n",
      "['food', -3.2580965380214821, -2.3513752571634776]\n"
     ]
    }
   ],
   "source": [
    "p0V,p1V,pAb=trainNB0(trainMat,listClasses)\n",
    "for i in range(len(myVocabList)):\n",
    "    print [myVocabList[i],p0V[i],p1V[i]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 베이지안 분류기 구현\n",
    " 1. 문장(문서)에서의 모든 단어들에 대해서 부정적인 문서에서 나올 확률 계산\n",
    " 2. 문장(문서)에서의 모든 단어들에 대해서 긍정적인 문서에서 나올 확률 계산\n",
    " 3. 두개의 확률을 비교해서 더 높은 쪽으로 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classifyNB(vec2Classify, p0Vec, p1Vec, pClass1):\n",
    "    p1 = sum(vec2Classify * p1Vec) + log(pClass1)    #element-wise mult\n",
    "    p0 = sum(vec2Classify * p0Vec) + log(1.0 - pClass1)\n",
    "    if p1 > p0:\n",
    "        return 1\n",
    "    else: \n",
    "        return 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love', 'my', 'dalmation'] classified as:  0\n",
      "['stupid', 'garbage'] classified as:  1\n"
     ]
    }
   ],
   "source": [
    "listOPosts,listClasses = loadDataSet()\n",
    "myVocabList = createVocabList(listOPosts)\n",
    "trainMat=[]\n",
    "\n",
    "for postinDoc in listOPosts:\n",
    "    trainMat.append(setOfWords2Vec(myVocabList, postinDoc))\n",
    "    \n",
    "p0V,p1V,pAb = trainNB0(array(trainMat),array(listClasses))\n",
    "testEntry = ['love', 'my', 'dalmation']\n",
    "thisDoc = array(setOfWords2Vec(myVocabList, testEntry))\n",
    "print testEntry,'classified as: ',classifyNB(thisDoc,p0V,p1V,pAb)\n",
    "testEntry = ['stupid', 'garbage']\n",
    "thisDoc = array(setOfWords2Vec(myVocabList, testEntry))\n",
    "print testEntry,'classified as: ',classifyNB(thisDoc,p0V,p1V,pAb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 스팸 이메일 분류하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 텍스트 토큰 만들기\n",
    "- 이전 절에서 다루었던 단어 벡터를 텍스트 문서로부터 어떻게 생성하는지\n",
    "- 파이썬에서 스트링을 처리하는 메소드인 .split()을 사용하여 텍스트를 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'book', 'is', 'the', 'best', 'book', 'on', 'Python', 'or', 'M.L.', 'I', 'have', 'ever', 'laid', 'eyes', 'upon.']\n"
     ]
    }
   ],
   "source": [
    "MySent = 'This book is the best book on Python or M.L. I have ever laid eyes upon.'\n",
    "print MySent.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 문제점: 구두점이 단어의 일부로 간주됨\n",
    "- 해결책: 단어나 숫자에 상관없이 문장을 사용하도록 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'book', 'is', 'the', 'best', 'book', 'on', 'Python', 'or', 'M', 'L', 'I', 'have', 'ever', 'laid', 'eyes', 'upon', '']\n"
     ]
    }
   ],
   "source": [
    "# 구두점(.) 제거\n",
    "import re\n",
    "regEx = re.compile('\\\\W*')\n",
    "listOfTokens = regEx.split(MySent)\n",
    "print listOfTokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 스팸 이메일 분류 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['codeine', '15mg', 'for', '203', 'visa', 'only', 'codeine', 'methylmorphine', 'narcotic', 'opioid', 'pain', 'reliever', 'have', '15mg', '30mg', 'pills', '15mg', 'for', '203', '15mg', 'for', '385', '15mg', 'for', '562', 'visa', 'only']\n"
     ]
    }
   ],
   "source": [
    "def textParse(bigString):    ## 큰 문자열을 단어로 쪼개어준다.\n",
    "    import re\n",
    "    regEx = re.compile('\\\\W*')\n",
    "    listOfTokens = regEx.split(bigString)\n",
    "    wordList = []\n",
    "    \n",
    "    for tok in listOfTokens:\n",
    "        if (len(tok)>2):\n",
    "            wordList.append(tok.lower())\n",
    "            \n",
    "    return wordList\n",
    "\n",
    "wordList = textParse(open('email/spam/1.txt').read())\n",
    "print wordList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification error ['benoit', 'mandelbrot', '1924', '2010', 'benoit', 'mandelbrot', '1924', '2010', 'wilmott', 'team', 'benoit', 'mandelbrot', 'the', 'mathematician', 'the', 'father', 'fractal', 'mathematics', 'and', 'advocate', 'more', 'sophisticated', 'modelling', 'quantitative', 'finance', 'died', '14th', 'october', '2010', 'aged', 'wilmott', 'magazine', 'has', 'often', 'featured', 'mandelbrot', 'his', 'ideas', 'and', 'the', 'work', 'others', 'inspired', 'his', 'fundamental', 'insights', 'you', 'must', 'logged', 'view', 'these', 'articles', 'from', 'past', 'issues', 'wilmott', 'magazine']\n",
      "classification error ['yay', 'you', 'both', 'doing', 'fine', 'working', 'mba', 'design', 'strategy', 'cca', 'top', 'art', 'school', 'new', 'program', 'focusing', 'more', 'right', 'brained', 'creative', 'and', 'strategic', 'approach', 'management', 'the', 'way', 'done', 'today']\n",
      "the error rate is:  0.2\n"
     ]
    }
   ],
   "source": [
    "docList=[]; classList = []; fullText =[]\n",
    "for i in range(1,26):\n",
    "    wordList = textParse(open('email/spam/%d.txt' % i).read()) ## 스팸 이메일 txt를 읽어와 단어로 분리\n",
    "    docList.append(wordList) ## 단어 추가\n",
    "    fullText.extend(wordList) ## \n",
    "    classList.append(1) ## classList에 1 추가 (spam문서의 클래스는 1)\n",
    "    wordList = textParse(open('email/ham/%d.txt' % i).read()) ## 이메일 txt를 읽어와 단어로 분리\n",
    "    docList.append(wordList) ## 단어 추가\n",
    "    fullText.extend(wordList) \n",
    "    classList.append(0) ## classList에 0 추가 (이메일 문서의 클래스는 0)\n",
    "    \n",
    "vocabList = createVocabList(docList)# 단어 list 생성\n",
    "trainingSet = range(50); testSet=[]           #훈련 집합 생성\n",
    "\n",
    "## 10개의 난수 생성후 해당 인덱스를 테스트셋으로 분리\n",
    "\n",
    "for i in range(10): \n",
    "    randIndex = int(random.uniform(0,len(trainingSet))) ##교차검증을 위한 난수생성\n",
    "    testSet.append(trainingSet[randIndex]) ## 생성된 난수의 인덱스의 단어를 테스트 셋으로 선정\n",
    "    del(trainingSet[randIndex]) ## 테스트셋이 아닌 단어들은 훈련을 위해 사용 되므로 단어 리스트에서 훈련셋 제거\n",
    "\n",
    "trainMat=[]; trainClasses = []\n",
    "\n",
    "for docIndex in trainingSet:  #####testNB0를 이용하여 훈련  \n",
    "    trainMat.append(setOfWords2Vec(vocabList, docList[docIndex])) ## 훈련용 문서 리스트 생성\n",
    "    trainClasses.append(classList[docIndex])## 훈련용 문서와 같은 위치에 클래스 배열 생성\n",
    "    \n",
    "p0V,p1V,pSpam = trainNB0(array(trainMat),array(trainClasses))## 트래이닝 여기에서 pSpam은 사전확률\n",
    "errorCount = 0\n",
    "for docIndex in testSet:        ## 나머지 set을 이용하여 분류\n",
    "    wordVector = setOfWords2Vec(vocabList, docList[docIndex])## 분류용 문서를 벡터 형태로 변환 \n",
    "    if classifyNB(array(wordVector),p0V,p1V,pSpam) != classList[docIndex]: ## 분류결과가 실제 결과와 같지 않은 경우\n",
    "        errorCount += 1 ## 에러 추가\n",
    "        print \"classification error\",docList[docIndex]\n",
    "        \n",
    "print 'the error rate is: ',float(errorCount)/len(testSet) ## 에러율 출력"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
